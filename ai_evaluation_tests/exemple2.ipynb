{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All needed import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import needed for the pre-processing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model import\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for the logs\n",
    "from utils.mlflow_logs import log_confusion_matrix, log_fn_and_fp, log_f1_score, log_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "RANDOM_STATE = 42\n",
    "SEED = 42\n",
    "URI = config(\"URI\")\n",
    "EXPERIMENT_ID = \"356734765655118797\"  # TODO put your experiment id here (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the datasets\n",
    "DATA_PATH = \"datasets/ds_salaries.csv\"  # TODO put the path to your dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FUNCTION = {\n",
    "    \"lightGBM with poisson loss 1\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.01, tol=1e-7),\n",
    "    \"lightGBM with poisson loss 2\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.05, tol=1e-7),\n",
    "    \"lightGBM with poisson loss 3\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.1, tol=1e-7),\n",
    "    \"lightGBM with poisson loss 4\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.01, tol=1e-5),\n",
    "    \"lightGBM with poisson loss 5\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.05, tol=1e-5),\n",
    "    \"lightGBM with poisson loss 6\" : HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.1, tol=1e-5),\n",
    "    \"lightGBM with abs loss 1\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.01, tol=1e-7),\n",
    "    \"lightGBM with abs loss 2\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.05, tol=1e-7),\n",
    "    \"lightGBM with abs loss 3\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.1, tol=1e-7),\n",
    "    \"lightGBM with abs loss 4\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.01, tol=1e-5),\n",
    "    \"lightGBM with abs loss 5\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.05, tol=1e-5),\n",
    "    \"lightGBM with abs loss 6\" : HistGradientBoostingRegressor(loss=\"absolute_error\", learning_rate=0.1, tol=1e-5),\n",
    "    \"multi layer perceptron 1\" : MLPRegressor(hidden_layer_sizes=100, activation=\"identity\"),\n",
    "    \"multi layer perceptron 2\" : MLPRegressor(hidden_layer_sizes=100, activation=\"logistic\"),\n",
    "    \"multi layer perceptron 3\" : MLPRegressor(hidden_layer_sizes=100, activation=\"tanh\"),\n",
    "    \"multi layer perceptron 4\" : MLPRegressor(hidden_layer_sizes=100, activation=\"relu\"),\n",
    "    \"multi layer perceptron 5\" : MLPRegressor(hidden_layer_sizes=150, activation=\"identity\"),\n",
    "    \"multi layer perceptron 6\" : MLPRegressor(hidden_layer_sizes=150, activation=\"logistic\"),\n",
    "    \"multi layer perceptron 7\" : MLPRegressor(hidden_layer_sizes=150, activation=\"tanh\"),\n",
    "    \"multi layer perceptron 8\" : MLPRegressor(hidden_layer_sizes=150, activation=\"relu\"),\n",
    "    \"Adaboost 1\" : AdaBoostRegressor(n_estimators=50, loss=\"linear\"),\n",
    "    \"Adaboost 2\" : AdaBoostRegressor(n_estimators=50, loss=\"square\"),\n",
    "    \"Adaboost 3\" : AdaBoostRegressor(n_estimators=50, loss=\"exponential\"),\n",
    "    \"Adaboost 4\" : AdaBoostRegressor(n_estimators=100, loss=\"linear\"),\n",
    "    \"Adaboost 5\" : AdaBoostRegressor(n_estimators=100, loss=\"square\"),\n",
    "    \"Adaboost 6\" : AdaBoostRegressor(n_estimators=100, loss=\"exponential\"),\n",
    "    \"Ridge 1\" : Ridge(alpha=0.25),\n",
    "    \"Ridge 2\" : Ridge(alpha=0.5),\n",
    "    \"Ridge 3\" : Ridge(alpha=1),\n",
    "    \"Ridge 4\" : Ridge(alpha=1.5),\n",
    "    \"Ridge 5\" : Ridge(alpha=2),\n",
    "}  # TODO create a dictionary for all : model_name -> class    /!\\ each model must have a different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get and pre-process the datas\n",
    "def get_data(frac: float = 1.0) -> Tuple:\n",
    "    \"\"\"Function used for the weather dataset\"\"\"\n",
    "\n",
    "    data = pd.read_csv(DATA_PATH).sample(frac=frac, random_state=RANDOM_STATE)\n",
    "    target_column = \"salary_in_usd\" # TODO give here the target column\n",
    "    data = data.drop([\"salary_currency\", \"salary\"], axis=1)  # TODO drop here the unecessary column\n",
    "    for column in [\"experience_level\", \"employment_type\", \"job_title\", \"employee_residence\", \"company_location\", \"company_size\"]:  # TODO column to transform in numerical values\n",
    "        data[column] = LabelEncoder().fit_transform(data[column])\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    iforest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outliers = iforest.fit_predict(data)\n",
    "    clean_data = data[(outliers != -1)]\n",
    "\n",
    "    # we normalize\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    clean_array = min_max_scaler.fit_transform(clean_data)\n",
    "    clean_data = pd.DataFrame(clean_array, columns=clean_data.keys())\n",
    "\n",
    "    data_values = clean_data.drop([target_column], axis=1)\n",
    "    data_target = clean_data[target_column]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data_values, data_target, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_params(X_train, X_test, model_name) -> None:\n",
    "    mlflow.log_param(\"nb_features\", X_train.shape[1])\n",
    "    mlflow.log_param(\"nb_samples_train\", X_train.shape[0])\n",
    "    mlflow.log_param(\"nb_samples_test\", X_test.shape[0])\n",
    "    mlflow.log_param(\"model_name\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.set_tracking_uri(URI)\n",
    "    mlflow.sklearn.autolog()\n",
    "    frac = 1  # TODO put here the wanted part of the total dataset to use (between 0 and 1)\n",
    "    print(\"data loading\")\n",
    "    (X_train, Y_train), (X_test, Y_test) = get_data(frac)\n",
    "    for model_name in MODEL_FUNCTION:\n",
    "        run_name = f\"Run of {model_name}\"  # TODO you can change the name of the form here\n",
    "        with mlflow.start_run(run_name=run_name, experiment_id=EXPERIMENT_ID):\n",
    "            model = MODEL_FUNCTION[model_name]\n",
    "            model.fit(X_train, Y_train)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "            log_params(X_train, X_test, model_name)  # this line is optional\n",
    "            model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "            eval_data = X_test\n",
    "            eval_data[\"label\"] = Y_test\n",
    "            mlflow.evaluate(\n",
    "                model=model_uri,\n",
    "                data=eval_data,\n",
    "                targets=\"label\",\n",
    "                model_type=\"regressor\",  # TODO complete here the type of model (\"regressor\" or \"classifier\")\n",
    "                evaluators=\"default\",\n",
    "            )\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/24 16:45:57 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9cbf5c3105794cc582192e45df82cb16', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "2023/05/24 16:45:58 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2023/05/24 16:45:59 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\hennecarta\\Anaconda3\\lib\\site-packages\\mlflow\\models\\signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2023/05/24 16:46:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:46:32 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:46:35 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:46:53 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:46:53 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:46:56 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:47:14 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:47:14 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:47:17 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:47:35 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:47:35 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:47:37 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:47:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:47:56 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:47:59 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:48:16 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:48:16 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:48:21 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:48:40 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:48:40 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:48:43 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:49:00 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:49:00 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:49:04 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:49:21 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:49:21 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:49:25 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:49:41 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:49:41 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:49:44 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:50:00 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:50:00 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:50:03 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:50:23 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:50:24 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Tree is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:50:27 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'TreeEnsemble' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:50:44 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:50:44 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='identity', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:50:58 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:50:58 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='logistic', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:51:12 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:51:12 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='tanh', hidden_layer_sizes=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:51:27 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:51:27 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(hidden_layer_sizes=100)'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:51:42 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:51:42 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='identity', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:51:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:51:56 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='logistic', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:52:09 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:52:09 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(activation='tanh', hidden_layer_sizes=150)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:52:22 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:52:23 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPRegressor(hidden_layer_sizes=150)'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:52:36 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:52:36 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor()'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:52:50 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:52:50 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor(loss='square')\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:53:08 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:53:08 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor(loss='exponential')\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:53:22 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:53:22 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError('The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor(n_estimators=100)'). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:53:36 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:53:36 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor(loss='square', n_estimators=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:53:49 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:53:49 WARNING mlflow.models.evaluation.default_evaluator: Shap evaluation failed. Reason: TypeError(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostRegressor(loss='exponential', n_estimators=100)\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:54:05 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:54:05 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:54:05 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:54:22 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:54:23 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:54:23 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:54:39 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:54:39 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:54:39 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:54:57 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:54:57 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:54:58 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/24 16:55:15 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/05/24 16:55:15 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/05/24 16:55:15 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'Ridge' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "main()  # We launch it all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
